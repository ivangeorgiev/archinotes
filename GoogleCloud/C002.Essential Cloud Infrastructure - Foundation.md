---
typora-root-url: ./
typora-copy-images-to: copied
---



## Introduction to GCP Infrastructure

* GCP Infrastructure
* Using GCP
* Labs and Demos
* Quiz

### GCP Infrastructure

![](/res/001.GoogleCloudPlatform.png)



3 core features

* infrastructure
* platform
* software

set of cloud-based products and services

It's important to understand that there's usually more than one solution for a task or application GCP. To better understand this, let's look at a solution continuum. 



![](/res/002.Solution Continuum.png)

Google Cloud Platform spans from infrastructure as a service, through platform as a service, to software as a service. Notice that each alternative solution on this continuum causes concern about different objects like managing components, systems or resources. This changes the role of the operation staff like IT Ops, SysOps, all the way through No ops, and changes the items that are being managed like CPUs, memories and disks or servers applications with auto-scaling. Throughout this infrastructure specialization, we'll be covering this entire solution continuum.

Let's start with the infrastructure. 

![](/res/003.Infrastructure Analogy.png)

An IT infrastructure is like a city infrastructure. The infrastructure is the basic underlying framework of fundamental facilities, and systems such as transport, communications, power, water, fuel and other essential services. The people in the city are like users, and the cars, bikes and buildings in the city are like applications. Everything that goes into creating and supporting those application slash buildings for the users slash citizens is the infrastructure. The purpose of this specialization is to explore as efficiently and clearly as possible the infrastructure services provided by GCP. You should become familiar enough with the infrastructure services that you will know what the services do, and basically how to use them. By the end of this class, you will be prepared to learn anything that you need to do to use the Google Cloud Platform. In other words, we won't go into a very deep dive case studies on specific vertical applications, but you'll know enough to put all the building blocks together to build your own solution. 

![](/res/004.Cloud Infrastructure.png)

Specifically, this series of courses consists of four parts. First, we will cover the foundation of the essential infrastructure which will cover the basic technologies. Next, we will focus on the core services which are the building blocks of the central infrastructure. Then, we will cover the augmented infrastructure which are the systems built on top of the essential infrastructure for scaling and automation Finally, we will focus on the application infrastructure consisting of containers and the services specifically provided to make it easier to develop applications for your users. The individual bullet points represent each module that we will cover. 

### Using GCP

Let's look at how to use GCP. There are three basic ways you can interact with the Google Cloud Platform. You can use the Google Cloud Platform Console, which provides a web-based graphical user interface that you access through console.cloud.google.com. If you prefer to work in the terminal window, the Google Cloud SDK provides the gcloud command-line tool. GCP also provides Cloud Shell, which is a browser-based interactive shell environment for GCP that you can access from the GCP Console. 

![](/res/005.Google Cloud Shell.png)

Cloud Shell is a temporary virtual machine with five gigabytes of persistent disk storage that has the Google Cloud SDK pre-installed. Throughout this specialization, you will apply what you learn in different labs. 

![](/res/006.Lab Conventions - Console.png)

These labs will have instructions to use the GCP Console such as, on the products and services menu, click "Compute Engine", "VM instances". Let's dissect these instructions. First, within the GCP Console, you will click on the icon with the three horizontal lines which is the "Products and Services" icon, as you can see on the left. This opens a menu, as you can see on the right. All of the major products and services are listed in this menu. Second, within the menu, hover over "Compute Engine" to open a submenu. Finally, click on "VM instances" within the submenu. You will get more comfortable with these instructions and the GCP Console as you work on labs. 

![1541934894359](/copied/1541934894359.png)

Labs will also use command-line instructions. You will enter these instructions either in Cloud Shell or an SSH terminal by simply copying and pasting them. The right side of the slide shows an example of what a command and its output will look like. In some cases, you will have to modify these commands, for example, when choosing a global unique name for a Cloud Storage bucket. 

![1541934978066](/copied/1541934978066.png)

Besides the Cloud SDK, you will also use client libraries that enable you to easily create and manage resources. GCP Client Library expose APIs for two main purposes. App APIs provide access to services, and they're optimized for supported languages such as NodeJS and Python. Admin APIs offer functionality for resource management. For example, you can use admin APIs if you want to build your own automated tools. 



## Module 1: Labs and Demos

### Lab 1: Console and Cloud Shell (Overview and Objectives)

Slides are a great way for explaining concepts but let's apply what we just talked about. 

![1541935123479](/copied/1541935123479.png)

In this first lab, you'll explore the GCP interface and the entry point of the graphical user interface that's called the Console. Within the console, you will create a storage bucket in cloud storage which is Google's unified object storage. Then you will repeat the same task using Cloud Shell which is a text-based user interface in GCP. 


You're encouraged to develop familiarity with both Console and Cloud Shell and to become comfortable moving back and forth between them. 

The lab will take about 20 minutes to complete, but you have 40 minutes before it times out. 



### Getting Started with Google Cloud Platform and Qwiklabs

3 min

Hello, and welcome. I’m Philipp Maier, a course developer with Google Cloud Platform. And this is a brief tutorial on using Qwiklabs in this course. I’m about to show you the interactive hands-on labs platform called Qwiklabs, which is part of Google Cloud. Qwiklabs allows you to get practical, hands-on experience with GCP and provisions you with Google account credentials so you can access the GCP console at no cost. The first step is to log in to Coursera in an incognito window. 

![1541935966009](/copied/1541935966009.png)

Depending on your browser, it may also be called private browsing or in-private browsing. Logging into Coursera from a private window, ensures that you don't accidentally use your own Google account while accessing the Google Cloud console. We don't want you to get any unexpected bills at the end of the month. Check out the links below this video for different browser support articles. Once logged into Coursera using an incognito window, return to your course and go to the lab activity page. 

![1541936024344](/copied/1541936024344.png)


So if prompted, you want to accept the honor code and you might even have to enter your name. Then you want to click the Open Tool button to open the lab in a new tab. 

So within the new tab you can now click Start Lab. And you want to wait until the lab running is displayed. 

![1541936090629](/copied/1541936090629.png)

For each lab, you will have a timer in the top right with the remaining access time. Your lab will automatically end when the timer runs out. On the left you have the connections detail. Click Open Google Console and then, sign in with the username and password provided in the Connections Detail pane. 

![1541936166792](/copied/1541936166792.png)

So I'm going to copy the username, paste that in here. I'm also going to take the password, Paste that in here too. Now, Qwiklabs creates a new account for you each time you launch a lab. Therefore, you need to click through the initial account setup windows. So essentially I need to accept this. I don't need to add any recover phone numbers, so I can just click Done. 

I'm going to agree to the Terms of Service and I don't need any emails. 

![1541936324593](/copied/1541936324593.png)



Now I can verify that I'm using the Qwiklabs provided account and project within the GCP Console. So up here I see the project ID.

And over here I can see the username. And these are the same ones that I was provided with in the Connections Detail pane.

You can also see that the timer is still running. Now, some labs track your work within the Qwiklabs provided GCP project. If this is enabled, you'll see a score in the top-right corner of the Qwiklabs window, as you can see right here. Your score increases as objectives are met, and you can click on the score to view the individual steps to be scored. 

Now that I've completed the lab I can see my score has been updated and I'm ready to click End Lab. Once I click End Lab the Qwiklabs provided project and any resources within that project will be deleted. 

I can close the Qwiklabs lab page, and my grade will be updated with my lab score in Coursera. That's it for this tutorial. Remember to launch Coursera in an incognito window, and use the Qwiklabs provided credentials to sign into the GCP console. Good luck with the labs, and enjoy the rest of this course. 





### Lab 1: Console and Cloud Shell (Review)

40 sec

In this lab, you created a Cloud Storage bucket using both Cloud Console and Cloud Shell within GCP. Cloud Console can do things Cloud Shell can't do and vice versa. Cloud Console can keep track of the context of your configuration activities, it can use the Cloud API to determine from current system state what options are valid, and it can perform repetitive and more leveraged activities on your behalf. Cloud Shell on the other hand, offers detailed and precise control and through its command, a way to script and automate activities. That being said, don't think of them as alternative, think of them as one extremely and flexible and powerful interface. 

### Demo: Projects

7 min

Next, we will explore projects which are the key organizer of infrastructure resources and relate these resources to billing accounts. Resources can only be created and consumed within projects in a way that projects isolate related resources from one another. You will see how to create, delete, and switch context between projects. Some of these actions cannot be performed in the Quick Labs environment due to security restrictions. Therefore, I'm going to demonstrate them in my environment. So, here I am in the GCP Console, and if we see up here where it says My First Project, I'm going to click on that and this shows me the current projects that I have created. Now, to go ahead and create a new project, I'm just going to hover over here on the plus icon and that is clearly identifying as the way to create a project. Now, first, I'm asked for a project name, this is really just a human-readable name, a way for you to identify your projects within your console and it's not used by any of the Google APIs. So, let me just say, This is My Awesome Project. When I do that, you'll see that down here there's automatically a project ID which is being generated.

![1541939625299](/copied/1541939625299.png)

Now, this, on the other hand, is globally unique, meaning that no one else can be using that project ID. If you don't like your project name, you can always go ahead and change that after you have created your project, but the project ID will stay this value once you've created your project. So, let me go ahead and click Create, and this is going to pop up a notification up here, and it's just can say that is currently creating my project and also let me know once that is completed. So, we see it's actually completed, so let me go ahead and switch to that new project which I do through the same interface that I use to create the project. Now, I can go to the project settings, and here as you can see, I can change the project name as I mentioned earlier, and I also have my project ID and there's also a project number. This is also a globally unique identifier and cannot be changed either once the project is created. Now, there are a lot of services associated with this project. So, for example, if I now want to go ahead and create a Google Compute Engine instance, I would first have to activate that service, and that would pop up then a notification and let me know when this is active, and this really just takes a minute or two but it's a step that's required for every new project that you create. Now, once you don't need a project anymore, you could go and shut it down. The button is right up here for that to shut down, and really what that does is it request the project to be deleted, specifically, within 30 days. So, if I processes right now, it's telling me that the billing and traffic serving will stop, I will lose access to the entire project, and all the project owners will be notified and can stop the shut down within the 30-day window. In my case here, I want to actually go ahead and shut this down. To do that, I need to verify the shutdown procedure with the project ID. So, I'm just going to type that down here. 

So, it's also showing me that ID so I can directly copy that. I click on Shut Down. This is going to process now and here I get a notification of when exactly that project is going to be deleted, which is a month from today. So, let me go ahead and close that. So now we see, I just have the two projects in here that I originally had. Now, to do anything else, I really need to select a project, so let me select this first project. Now, you can also switch between projects using Cloud Shell, which you've seen in the previous lab. So, let me go ahead and Activate Cloud Shell. If this is the first time activating Cloud Shell, you'll get another notification saying do you want to Activate Cloud Shell? And this is now establishing a connection. So we're just going to wait for that to load up. Now I am within Cloud Shell for this specific project. We can see that up here in my session, it's currently saying the Project ID, which if I scroll down here, we see that as the project ID for that project. I can also type in a command to identify the current project, so let me do that. So the command I'm going to type is G Cloud config list and this going to list all the configurations, and specifically, we see that the project ID is listed here. Now, what happens if I go into the GCP Console and now switch to a different project? So, instead of this first project, let me switch to this other project, okay? So we can see the console itself has changed. But in Cloud Shell, if I now ask again for the project ID which I can actually be more specific, use the same command but also use the grep command and then specifically asked for the project, then it's going to tell me that I'm still within that first project ID. The other way you can tell it is if you look at the command line, you see that down here I first have the username at and then I have the project ID, and you also see it within the session up here, the first project ideas cronies delisted. So, how do we change that? Well, you could close the Cloud Shell right now and reopen it or you could open a new session. So, I can do it here by clicking on this plus icon, and if in here now I asked for the project, we can see that I'm looking at that second project. Another way to do this is you can actually just switch the project directly within the command line. A good strategy is always to store some of the IDs and project numbers that you have within a shell variables so let's go ahead and do that. I'm going to create a variable called PROJECT_1_ID and I'm going to get that project ID. Just copy it right here and paste it directly into Cloud Shell. So now, I have a variable that is holding that. Now, the command to actually change the project is Gcloud config set project, and I'm just going to use that variable then. So, Project_1_ ID, and this says it update it now. We can see that because the command line here has updated, and I can also again query for that project, and we can see that now my console is that project and my Cloud Shell is also changed. So, in this demo, I showed you how projects are created and deleted, how to switch context between projects, and how to use Cloud Shell with projects. Most importantly, the console's current project is independent from the Cloud Shell current projects. But as you saw, you can use the Cloud Shell command prompt as an indicator and you can issue a command to change the current project in Cloud Shell. 

### Lab 2: Infrastructure Preview (Overview and Objectives)

33 sec

![1541940141253](/copied/1541940141253.png)

In this lab, you're going to experience the power of GCP automation by setting up a complete Jenkins Continuous Integration environment using Cloud Launcher. You will then verify that you can manage the service from the Jenkins UI and administer the servers from the VM host through SSH. Now, you could accomplish a very similar result through manual configuration in a couple days, but in this lab you'll see it only takes a few minutes. Specifically, the lab will take about 15 minutes to complete, but you have 30 minutes before it times out. 

### Lab 2: Infrastructure Preview

15 min

### Lab 2: Infrastructure Preview (Review)

46 sec

In this lab, you were able to launch a complete continuous integration solution in just a few minutes. You demonstrated that you had user access through the Jenkins UI and that you had administrative control over Jenkins by using SSH to connect to the VM. Where the surface is hosted and by stopping and then restarting the services. Many of the activities that occurred in this lab, were nearly transparent and they use resources and methods that you'll learn about in the rest of the specialization. For example, the acquisition and configuration of a network IP address, the provisioning of a virtual machine instance along with the installation of software on that machine, and the passing of default state information from the environment during the setup process. 

### Review

#### Module 1 Quiz

3 questions

#### Module 1 Review

In this module, I gave you an overview of the Google Cloud Platform Infrastructure. Then, we looked at how to use GCP, which you got to experience in two short labs. I also gave you a demonstration of how to use projects, which are the key organizer of infrastructure resources. Now that you can interact with GCP, it's time to explore two of the foundational components of GCP's infrastructure, namely networking, and virtual machines. So, what are you waiting for? Move on to the next module to learn more. 





## Module 2: Virtual Networking

GCP uses a software defined network or STN. It's built on a global fibre infrastructure that makes GCP one of the world's largest and fastest networks. One way to view GCP is as a collection of network costed services. Thinking about resources as services rather than as hardware, will help you understand the options that are available and their behavior.


Learning Objectives

- Recall that a key distinguishing feature of networking in GCP is that network topology is not dependent on address layout.
- Differentiate the three different types of networks offered in GCP (default, auto, custom).
- Explain that firewall rules applied by tags enable advanced flexible access management.
- Design a maintenance server to gain access to and verify internal connectivity to an application server.



### Virtual Networking

This is module two: Virtual Networks. GCP uses a software defined network that is built on a global fiber infrastructure that makes GCP one of the largest and fastest networks. Thinking about resources as services rather than as hardware, will help you understand the options that are available and their behavior. For example, a persistent disk isn't really a physical device. It's a service that you acquire and use over a network. So, a good understanding of GCP begins with a solid understanding of how GCP has implemented networking. 

![1541943310707](/copied/1541943310707.png)

In this module, we will start by introducing Virtual Private Cloud or VPC which is Google's managed networking functionality for your Cloud Platform resources. Then, we are going to dissect networking into its fundamental components which are projects, networks, subnetworks, IP addresses, routes and rules along with billing. Next, you will explore GCP's network structure in a lab by creating networks and subnetworks of many different varieties and exploring the network relationships between them. After that, we will look at common network designs like a bastion host isolation which you will get to implement in a lab. 

#### Google Cloud Platform (GCP) VPC

![1541951526195](/copied/1541951526195.png)

With Google Cloud Platform VPC, you can provision your GCP resources, connect them to each other and isolate them from one another in a virtual private cloud. You can also define fine grained networking policies within GCP, and between GCP and on premises or other public clouds. Essentially, VPC is a comprehensive set of Google managed networking objects that we will explore in detail throughout this module. Projects are going to encompass every single service that you utilize. **Networks** come in three different flavors, default, auto mode and custom mode. **Subnetworks** allow you to divide or segregate your environment. **Regions** and **zones** represent Google's data centers and they provide continuous data protection and high availability. VPC provides **IP** addresses for internal and external use along with granular IP address range selections. As for virtual machines, in this module, we'll focus on configuring a VM from a networking perspective. In the next module, we'll go deeper into how to setup and configure virtual machines. Finally, we'll conclude with **routes** and **firewalls**. IP forwarding, protocol forwarding, load balancing, cloud DNS and VPN tunnels are built on top of these objects and are covered separately in a later course. 

#### Projects, networks, and subnetworks

![1541951711672](/copied/1541951711672.png)

Let's start by looking at projects, networks, and subnetworks. In the previous module, I introduced projects as the key organizer of infrastructure resources. Specifically, a project associates objects and services with billing. Now, what's unique is the fact that projects actually contain entire networks, up to five networks per project to be exact. These networks do not have IP ranges, but are simply a construct of all of the individual IP addresses, or services that are within that network. GCPs networks are global, spending all available regions across the world. So, you can have one network that literally exist anywhere in the world, Asia, Europe, America all simultaneously. Inside of a network, you can segregate your resources with subnetworks. And as I mentioned earlier, there are three different types of networks: default, auto mode, and custom mode. You'll explore these three types in the first lab of this module. However, let me preface that while an auto mode network can be converted to a custom mode network, a custom mode network cannot be converted to anything else. Essentially, once custom always custom. 

Let's look an example of how networks can isolate systems in GCP. 

![1541951852302](/copied/1541951852302.png)

On this flight we have an example of a project that contains five networks. All of these networks spend multiple regions across the world as you can see on the right hand side. Each network contains separate virtual machines. Since VMs A and B are on the same network, network one, they can communicate over internal IP addresses, even though they're in different regions. Essentially, your virtual machines even if they exist in different locations across the world, take advantage of Google's global fibre network. Those virtual machines are going to appear as though they're sitting in the same rack when it comes to a Network Configuration Protocol. VM C and D on the other hand, are not in the same network. Therefore, these VMs must communicate over external IPs, even though they're in the same region. The traffic between VM C and D isn't actually touching the Internet, but is going through the Google Edge routers which has different billing and security ramifications that we're going to explore. 

To better understand the slide, let me define regions and zones. 

![1541951952294](/copied/1541951952294.png)

A region is a specific geographical location where you can run your resources. Each region has one or more zones. For example, the US Central one region denotes a region in the central United States that has zones US Central 1A, US Central 1B, US Central 1C, and US Central 1F. For an up-to-date list of GCP regions and zones, please refer to the online documentation, as new regions and zones are constantly added. This slide has a region, region one with two zones, Zone A, and Zone B. Subnetworks can extend across zones within the same region such as subnet one. The subnet is simply an IP address range, so you can carve up IP addresses within that range. Notice that the first address in the range 10.0.0.1 is reserved for the router address. The last address in the range 10.0.0.255 is reserved for the broadcast address. Even though the two virtual machines in this example are across different zones, they will still communicate with each other using the same subnet IP address. Essentially, a single firewall rule will apply to both VMs even though they are in different zones. 

![1541952085488](/copied/1541952085488.png)

In an on-premises environment, you might design subnetworks works in the traditional way that is based on the stacks of routers or switches within your environment. Essentially, a top-down hierarchy, as you can see on the right hand side, where you have a master network 10.0, and you've curved it up into your core switches and core routers specifically into 10.1 and 10.5. Each of these might represent different environments such as marketing, ERP, or development. In GCP, you just simply set up a network, and you create multiple subnets for marketing ERP and development. As you can see, networks do not have IP ranges. So, subnetworks don't need to fit into an address hierarchy. While subnetworks can be used to manage resources, there are other ways of managing resources like Cloud IAM, labels, tags, and managed instance groups. These are covered throughout the different courses of the specialization. 

#### IP addresses

Now that we covered GCP networks at a high level, let's go deeper by exploring IP addresses. 

![1541952265021](/copied/1541952265021.png)



In GCP, each virtual machine can get two IP addresses assigned. One of them is an internal IP address, which is going to be assigned via DHCP internally. Every VM that starts up and any service that depends on virtual machines gets an internal IP address. Example of such services are Google App Engine and Kubernetes Engine which are explored in other courses. When you create VMs and GCP, their symbolic name is registered with an internal DNS service that translates the name to the internal IP address. DNS is scoped to the network, so it can translate web URLs and VM names of hosts in the same network, but it can't translate hostnames from VMs in a different network. The other IP address is the external IP address which is optional. In essence, you can assign an external IP address if your device or machine is externally facing. That IP address can either be assigned from a pool, making it ephemeral, or it can be assigned a reserved external IP address, making it static. Keep in mind that you're billed for reserving external IP addresses even when they're not attached to a running VM.

![1541952483417](/copied/1541952483417.png)

Regardless of whether you use an ephemeral or static IP address, the external address is unknown to the OS of the VM. The external IP address is mapped to the VM's internal address transparently by VPC. I'm illustrating this here by running IF config within a VM in GCP which only returns the internal IP address. Let's explore this further by looking at DNS resolution for both internal and external addresses.

Let's start with internal addresses.

![1541952582080](/copied/1541952582080.png)

Each instance has a hostname that can be resolved to an internal IP address. This hostname is the same as the instance name. There are also internal fully qualified domain name, or FQDN for an instance that uses the format hostname.c.project-id.internal as shown on the slide. If you delete and recreate an instance, the internal IP address can change. This change can disrupt connections from other Google Compute Engine resources, which must obtain the new IP address before they can connect again. Wherever the DNS name always points to a specific instance no matter what the internal IP address is. Each instance has a metadata server that also acts as a DNS Resolver for that instance. The metadata server handles all DNS queries for local network resources and routes all other queries to Google's public DNS servers for public name resolution. I previously mentioned that an instance is not aware of any external IP address assigned to it. Instead, the network stores look up table that matches the external IP address with the internal IP address of the relevant instance. 

Now let's look at external addresses.

![1541952735751](/copied/1541952735751.png)

Instances with external IP addresses can allow connections from hosts outside of the project. Users can do so directly using the external IP address. Public DNS records pointing to instances are not published automatically. However, admins can publish these using existing DNS servers. Domain names can be hosted on GCP using Google Cloud DNS. This is a managed service that is definitely worth considering. If you don't want to create your own BIND server and another VM. 

Another networking feature of GCP is alias IP ranges.

![1541952821031](/copied/1541952821031.png)

Alias IP ranges let's you assign a range of internal IP addresses as aliases to a virtual machine's primary network interface. This is useful if you have multiple services running on a VM and you want to assign each service a different IP address. In essence, you can configure multiple IP addresses representing containers or applications hosted on a VM without having to define a separate network interface. You just draw out the alias IP range from the local subnets primary or secondary CIDR ranges. This diagram provides a basic illustration of primary and secondary CIDR ranges and VM alias IP ranges. Configuring alias IP ranges describes commands for setting up a subnet with secondary ranges and for assigning alias IP addresses to VMs. 

#### Routes and rules

So far you've learned about projects, networks, subnetworks, and IP addresses. Let's use what you've learned to understand how GCP routes traffic. 

![1541953005011](/copied/1541953005011.png)

By default, every network has routes that let instances in the network sent traffic directly to each other even across subnets. In addition, every network has a default route that directs packets to destinations that are outside the network. Although these routes cover most of your everybody needs, you can also create special routes that override these routes. Just because a packet has a route to a destination does not mean that it can get there. Firewall rules must also allow the packet. The default network has preconfigured firewall rules that allow all instances in the network to talk with each other. Manually created networks do not have such rules. So, you must create them as you will experience in the first lab. 

![1541953159063](/copied/1541953159063.png)

Routes match packets by destination IP address. However, no traffic will flow without also matching a firewall rule. A route is created when a network is created, enabling traffic delivery from anywhere. Also, a route is created when the subnet is created. This is what enables VMs on the same subnet to communicate internally. This slide shows a simplified routing table, but let's look at this in more detail.

![1541953221582](/copied/1541953221582.png)

Each route in the route's collection may apply to one or more instances. A route applies to an instance, if the network and instance stacks match. If the network matches and there are no instance tags specified, the route applies to all instances in that network. Compute engine then uses the routes collection to create individual read-only routing tables for each instance. This diagram shows a massively scalable virtual router at the core of each network. Every virtual machine instance in the network is directly connected to this router, and all packets leaving a virtual machine instance are first handled at this layer before they are forwarded on their next stop. The virtual network router selects the next hop for packet by consulting the writing table for that instance. In this diagram, the green boxes are virtual machine instances, the router is the yellow at the center, and the individual routing tables are indicated by 10 boxes. 

![1541953351781](/copied/1541953351781.png)

GCP firewall rules protect your virtual machine instances from unapproved connections, both inbound and outbound known as ingress and egress respectively. Essentially, every VPC network functions as a distributed firewall. While firewall rules are applied to the network as a whole, connections are allowed or denied at the instance level. You can think of the firewall as existing not only between your incenses and other networks, but between individual instances within the same network. Also, if for some reason all firewall rules in a network are deleted, there's still an implied deny all ingress rule, and an implied allow all egress rule for the network.

You can express your desired firewall configuration as a set of firewall rules.

![1541953448232](/copied/1541953448232.png)

Conceptually, a firewall rule is composed of the following parameters. The direction of the rule. Inbound connections are matched against ingress rules only, and outbound connections are matched against egress rules only, the source of the connection for ingress packets or the destination of the connection for egress packets. There is the protocol and port of the connection, where any rule can be restricted to apply to specific protocols only or specific combinations of protocols and ports only. There is the action of the rule, which allows or denies packets to match the direction protocol port and source or destination of the rule, the priority of the rule which governs the order in which rules are evaluated, the first matching rule is applied, and the rule assignment. By default, all rules are assigned to all instances, but you can assign certain rules to certain instances only. 

Let's look at some GCP firewall use cases for both egress and ingress.

![1541953563382](/copied/1541953563382.png)

Egress firewall rules control outgoing connections originated inside your GCP network. Egress allow rules allow open connections that match specific protocols, ports, and IP addresses. Egress deny rules, prevent instances from initiating connections that match non-permitted port, protocol, and IP range combinations. For Egress firewall rules, destinations to which a rule applies may be specified using IP side or ranges. Specifically, you can use destination ranges to protect from undesired connections, initiated by a VM instance, toward an external destination. For example, as shown on the left, an external host. You can also use destination ranges to protect from undesired connections, initiated by VM instance toward specific GCP side arranges. For example, as shown in the middle, a VM in a specific subnet.

![1541953745973](/copied/1541953745973.png)

Ingress firewall rules protect against incoming connections to the instance from any source. Ingress allow rules, allow specified protocol, ports, and IP addresses to connect in. The firewall prevents instances from receiving connections, are non permitted ports and protocols. Rules can be restricted to only affect particular sources. Source CIDR ranges can be used to protect from undesired connections coming to an instance either from external networks or from GCP IP set of ranges. In addition, source text can be used to protect from undesired connections coming from specific VM instances that are tagged with a matching tag. This diagram illustrates a VM receiving a connection from an external address, and another VM receiving a connection from a VM in the same network. You can control ingress connections from a VM instance by constructing inbound connection conditions using source site arranges, protocols, ports, and source tags on instances. However, SourceTags can only be used for VM to VM connections such as the one shown here in the middle. 

#### Billing

Before you apply what you just learned, let's talk about networking billing. It is important that you understand under what circumstances you will be billed for GCP's network. 

This table is from GCP's documentation and it provides general network pricing.

![1541953936724](/copied/1541953936724.png)

First of all, ingress or traffic coming into GCP's network is not charged. All egress traffic to the same zone, to a different GCP service within the same region, or to other Google products like YouTube, Maps, Drive from a VM in GCP with a public or private IP address is not charged either. However, there is a charge for egress between zones in the same region and between regions. All these charges are for egress through internal IP addresses. There are different charges for egress through external IP address, regardless of whether the instances are in the same zone. 

![1541954018768](/copied/1541954018768.png)

Another thing to consider when designing your network is your throughput and round-trip latency between virtual machines. This is going to vary by location, so check your applications requirements against current specifics for VPC when choosing where to place your VMs. For example, VM to VM communication within a single zone has much more consistent performance than VM to VM communication between regions in a single continent or even across continents. Keep in mind that as VPC is constantly evolving, there are some features that are marked BETA. These features do not have a service level agreement or SLA and the online documentation states which features are currently in BETA. 

![1541954099904](/copied/1541954099904.png)

If you make design changes, you might have to delete networks and subnetworks. If that is the case, start by deleting your VMs and any related firewall rules. Then, depending on whether you have an auto type network or a custom type network, you might only be able to delete the entire network or you might be able to delete the subnetworks independently off the entire network. You will explore this in the next lab.

 

#### Lab 1: Virtual Networking (Overview and Objectives)

Let's apply some of the network features we just discussed in a lab. 

In this lab you will build and explore a complex GCP network structure. The lab will take about 45 minutes to complete, but you have 90 minutes before it times out. Let me illustrate the different tests of this lab with some network diagrams.

![1541954324170](/copied/1541954324170.png)

In the first part of the lab you will build this complex multiple network topology. It consists of the default network, an auto mode network, and a custom mode network, along with its associated subnetworks. You will launch VMs in the various regions and subnets. And having these VMs in a variety of locations will enable you to explore the connectivity across and within multiple networks. 

![1541954379960](/copied/1541954379960.png)

Finally, you will use ping, trace route and SSH to test connectivity between the instances. You will then modify the firewall rules to meet policy requirements, and test to verify that the changes worked. 

![1541954430301](/copied/1541954430301.png)

In most labs you chose regions and zones where objects are located. However, this one is prescriptive about the network layout because it is systematically highlighting the differences between placing instances in a variety of network locations. 



#### Lab 1: Virtual Networking (Review)

38 sec



### Common Network Designs

#### Common network designs

![1542007010414](/copied/1542007010414.png)



Let's use what we have learned so far and look at common network designs. Specifically, we have gone over projects, networks, subnetworks, regions, and zones. The question now is, how do all of these elements work together? In short, they provide a rich set of alternatives for managing groups of resources with varying availability and access control requirements. So, what does that mean? Well, basically you can now manage resources at a very granular level, depending on how specific you need to get. If you need to work things globally, you have that capacity to do so, but you can also restrict yourself to very finite resources. Let's start by looking at availability. 

![1542007448369](/copied/1542007448369.png)

If your application needs increased availability, you could place two virtual machines into multiple zones within the same subnetwork, as shown on the slide. 

Using a single subnetwork allows you to create a firewall rule against the subnetwork 10.2.0.0. Therefore, by allocating VMs on a single subnet to separate zones, you get improved availability without additional security complexity. 

Next, let's look at globalization. 

![1542007516505](/copied/1542007516505.png)

In the previous design, we placed resources in different zones in a region, which provides isolation from many types of infrastructe, hardware and software failures. 

Putting resources in different regions, as shown on this slide, provides an even higher degree of fail independence. 

This allows you to design robust systems with resources spread across different failure domains. 

Also, using a load balancer, which we'll cover in a later course, you can route the traffic to the region that is closest to the user. 

Globalization does not get you the simplified security that we just saw with a single subnetwork, however, if the VMs are in a single network, as shown here, they can still communicate through GCP's internal global network. Let's take this separation one step further by placing resources in different regions within different networks and different projects. 

![1542007623855](/copied/1542007623855.png)



These resources are now isolated, which prevents compromise of one part from spreading to other parts, however, using VPC network peering, these resources can still communicate over a private address space. Finally, in this last case, the VMs are isolated into separate projects, but within the same zone. 

![1542007723550](/copied/1542007723550.png)



This can be useful for identity and access management, which is covered in detail in a later course. 

For example, if software development is Project 1, and test engineering is Project 2, you can assign different people to different roles in the projects for management separation. 

Consider dividing a system up into multiple projects for better access control, but remember that a network cannot span projects. So using separate projects implies that the VMs must communicate via the Internet.

Another common network design is a bastion host isolation. 

![1542007822559](/copied/1542007822559.png)



Bastion hosts provide an external face and point of entry into a network containing private network instances. 

This host can provide a single point of fortification or audit, and can be started and stopped to enable or disable inbound SSH communication from the Internet. For example, on this slide, Instance 1 represents a service provided to an internal corporate audience, therefore, this instance does not have an external IP address. 

In order to gain access to this instance, you can create a maintenance host known as a bastion host. 

You will configure such a network design in the upcoming lab and verify connectivity.

![1542174492321](/copied/1542174492321.png)

The last common network design is a network address translation, or NAT gateway isolation. 

Similar to the previous design we have an instance that does not have an external IP address. 

In this case, you can configure another instance in the same network as a NAT gateway with IP forwarding. 

It disallows Instance 1 to communicate with another instance on a separate network via the gateway. 

The two networks do not have to be in the same project for this design to work. 

#### Lab 2: Bastion Host (Overview and Objectives)

![1542174639916](/copied/1542174639916.png)

Let's apply one of the common network designs that we just discussed in the lab. In this lab you create a bastion host to enable maintenance access to your virtual machine. Specifically you will create an application web server to represent a service provided to an internal corporate audience than to prevent the web server from access to or from the internet, you will remove its external IP address. Finally, you will create the maintenance server to gain access to and verify internal connectivity to the application server. The lab will take about 20 minutes to complete but you have 40 minutes before it times out. 



## Module 3: Virtual Machines

### Overview

![1542256768578](/copied/1542256768578.png)



### Compute Engine

#### Compute Engine

There is a spectrum of different options in GCP when it comes to compute and processing. We're going to focus on the traditional virtual machine instances in this module.

![1542258230801](/copied/1542258230801.png)

All of the other items that you see here are going to be covered in data courses. Now, the difference is compute engine gives you the utmost inflexibility, run whatever language you want, it's your virtual machine. This is purely an infrastructure as a service model. You have a VM and an operating system and it is up to you to manage it and handle aspects such as auto scaling, where you'll configure the rules about adding more virtual machines in specific situations. Auto scaling will be covered in the scaling automation course of this specialization. The primary work case of compute engine is pretty much any general workload especially enterprise applications that were designed to run on a server infrastructure. This makes compute engine very portable and easy to run in the cloud. Other services like Kubernetes engine, which consist of containerized workloads may not be as easily transferable as what you're used to from On-premises. 

So, what is compute engine?

![1542258356044](/copied/1542258356044.png)

At its heart, it's infrastructure as a service. It's physical servers that you are used to running inside the Google environment with a number of different configurations. Both predefined and custom machine types allow you to choose how much memory and how much CPU you want. You choose the type of disk you want, whether you want to just use standard hard drives, SSDs, local SSDs, or a mix. You can even configure the networking and run a combination of Linux or Windows machines. 

In this module, we're going to cover several different features such as machine rightsizing, startup scripts, metadata, availability policies, and pricing and usage discounts. 

![1542258425335](/copied/1542258425335.png)

Let's start by looking at the compute options. 

![1542258475224](/copied/1542258475224.png)

Compute engine provides a number of different machine types. Specifically, there are predefined high CPU, high memory, standard, and shared core machine types. If those machine types don't fit your need, you can also customize your own machine. Your choice of CPU will affect your network throughput, specifically your network will scale at two gigabits per second for each CPU core up to a maximum of 16 gigabits per second worth of throughput, which is going to be achieved if you have eight virtual CPUs. When you're migrating from an On-premises world, you use the physical cores which have hyper threading. In GCP we call these vCPUs or virtual CPUs. A vCPU is equivalent to one hyper threaded core. Therefore, if you have a single core hyper threaded CPU On-premises that would essentially be two virtual CPUs to one physical core. So, always keep that in mind as oftentimes, people will immediately do a test. They'll say, "I have a four core physical machine and I'm going to run four cores in the Cloud." And ask why their performance isn't the same. 

Once you pick your compute options, you want to choose your disk.

![1542258578523](/copied/1542258578523.png)

You have three options; standard, SSD, or local SSD. So basically, do you want the standard spinning hard disk drives, HDDs or flash memory solid state drives, SSDs. Both of these options provide the same amount of capacity in terms of the size. Therefore, the question really is about performance versus cost as there's a different pricing structure model. Basically, SSDs are designed to give you a high number of IOPS per dollar versus standard which will give you a high amount of capacity for your dollar. Local SSDs have an even higher throughput and lower latency than SSD persistent disks because they're attached to the physical hardware. However, the data that you store on a local SSD persist only until you stop or delete the instance. Typically, a local SSD is used as a swap disk, just like you would if you wanted to create a round disc but if you need more capacity, you can store those on a local SSD. You can create instances with up to eight separate 375 gigabyte local SSD partitions for a total of three terabyte of local SSD space for each instance. Standard and non-local SSDs can be sized up to 64 terabytes for each instance. The performance of these disks scales with each gigabyte of space allocated. 

As for networking, we've already seen networking features applied to compute engine in the previous modules lab. 

![1542258942954](/copied/1542258942954.png)

We looked at the different types of networks and created firewall rules using IP addresses and network tags. Something else you'll notice is that you can do regional HTTPS load balancing and network load balancing. This doesn't require any pre-warming because again, this isn't a specific hardware device that needs to analyze your traffic. These are simply traffic engineering rules. They're coming into Google's network and VPC is applying your rules destined for your IP address of that range.



#### Demo: Compute Engine

Let's go to the GCP console and look at the compute and disk options that we just discussed. So, on the Products & services menu, I'm going to navigate to Compute Engine and then VM instances. Here I'm going to click Create. Now, this shows us the Create instance page and if we see here on the Machine type, there is a drop-down menu and I see all the predefined machine types.

![1542259073382](/copied/1542259073382.png)

Notice that we have the micro and small CPU configuration. Now, in these cases, these are shared CPU. They have completely different characteristics than the dedicated virtual machines and we will discuss those in more detail later. 

Now, in case the standard types don't work out for you, you can also customize your machine. 

![1542259121339](/copied/1542259121339.png)



So, over here by clicking on Customize, you can see that you can actually define the numbers of core or the amount of memory that you want to have. If I go down, I also have the option to change the Boot disk. So, if I click on the Change button here, you'll see that I have different images here. 

![1542259191983](/copied/1542259191983.png)

We're going to discuss images a little bit later in the module, but for now, under Boot disk type, I also have a drop-down list here and I see that I have both as options, the Standard persistent disk and the SSD persistent disk. So, I could choose that and I could define a size, let's say a 100 gigabyte. Now, that's going to be my boot disk, but I can also add additional discs. By expanding this section over here and then clicking on Disks, I have the option to add additional disks to my machine. So, I add an item, click on Select, and then click Create disk. Here, I now have all the three different disk types that we just discussed.

![1542259255241](/copied/1542259255241.png)

So, the new one that we have here is the Local SSD disk. And if I choose one of these and then define a size, let's say again a 100 gigabyte. You can see that I automatically get the estimated performance and when I increase that size, you'll see that the performance also updates along with that. So, that's how simple it is to go into the GCP console and create a virtual machine.

#### Pricing and discounts

When it comes to virtual machines, GCP offers a variety of different options in order to keep the prices low.

![1542448072681](/copied/1542448072681.png)



First, virtual machines are built per second, with a one minute minimum.

Second, Google offers **sustained use discounts**, which are automatic discounts that you get for running a VM instance for a significant portion of the billing month.

Compute Engine also offers **preemptible** instances, which come with up to an 80% discount. However, Compute Engine might terminate or preempt these instances with a 30 second notification, if it requires access to those resources for other tasks.

Compute Engine actually always terminates preemptible instances after they **run for 24 hours,** but I'll say more about preemptable instances later. The ability to customize the amount of memory in CPU through **custom machine types** allows to further pricing customization. Speaking of sizing your machine, compute engine provides VM sizing recommendations to help you optimize the research used of your virtual machine instances. When you create a new instance, recommendations for the new instance will appear 24 hours after the instance has been created.

Let's further explore VM charges and discounts.

![1542448285573](/copied/1542448285573.png)

Remember that Google offer sustained use discounts, which are automatic discounts that you get for running a VM instance for a significant portion of the billing month.

Specifically, when you run an instance for more than 25% of a month, Compute Engine automatically gives you a discount for every incremental second you use for that instance.

The discount increases with usage, and you can get up to 30% net discount for instances that run for the entire month.

These discounts are applied automatically, there's no action required on your part to enable these discounts.

Now, inferred instances means that for billing purposes the same type of machine used in the same zone will be combined into a single charge so that you get the most discount as if it were one machine in use the whole time.

And combined resources means that memory and virtual CPU of the same type are combined, so that you get the discount on the greatest resource consumption in custom-types. There are even deeper discounts for VM usage when you purchase committed use contracts, known as committed use discounts.

Compute Engine also has free usage limits, which, as of this recording, provides free usage of one f1-micro instance per month, with up to 30 gigabytes of HDD persistent disk. And up to one gigabyte of network egress. This example shows a customer's usage that comprises eight distinct instances, as we can see here, on the left.

![1542448457034](/copied/1542448457034.png)

The instances are combined to find the smallest number of simultaneous running instances, which are called inferred instances.

In this case, Compute Engine combines the instances to make four inferred instances with the longest possible duration, as we can see on the right. Compute Engine then calculates sustained use discounts based on the percentage of time that each of these inferred instances was running in a month.

#### VM access and lifecycle

When it comes to accessing a VM, the creator of an instance has full root privileges on that instance.

![1542448607605](/copied/1542448607605.png)



On a Linux instance, the creator has SSH capability and can use the GCP console to grant SSH capability to other users. On a Windows instance, the creator can use the GCP console to generate a username and password. After that, anyone who knows the username and password can connect to the instance using a remote desktop protocol or RDP client. I listed the required firewall rules for both SSH and RDP here, but you don't need to define these if you're using the default network that we covered in the previous module. 

![1542448660108](/copied/1542448660108.png)



The lifecycle of a VM is represented by different statuses. I will cover this lifecycle on a high level but I recommend referring back to this diagram as a reference. When you define all the properties of an instance and click create, the instance enters the provisioning state. Here the resources such as CPU, memory and disks are being reserved for the instance, but the instance itself isn't running it. Next, the instance moves to the staging state where resources have been acquired and the instance is prepared for launch. Specifically in this state the compute engine is adding IP addresses, booting up the system image, and booting up the system. 

Once the instance is running, it will go through pre-configured startup scripts and enable SSH or RDP access. Now, there are a lot of things you can do while your instance is running, for example, you can live migrate your virtual machine to another host in the same zone instead of requiring an instance to be rebooted. This allows Google to perform maintenance that is integral to keeping the infrastructure protected and reliable without interrupting any of your VMs. 

There are many other things you can do while your instance is running such as, moving your VM to a different zone, taking a snapshot of the VMs persistent disk, exporting the system image, or re-configuring metadata some of which we will explore in later lapse. Some actions require you to stop your virtual machine, for example, if you want to upgrade your machine by adding more CPU. 

When the instance enters the state, it will go through pre-configured shutdown scripts and end in the terminated state. 

From this state, you can choose to either restart the instance which would bring it back to its provisioning state, or you can delete it. You also have the option to reset a VM which is similar to pressing the reset button on your computer. This action wipes the memory contents of the machine and resets the virtual machine to its initial state. The instance remains in the running state through the reset. 

There are different ways in which we can change a VM state from running. 

![1542448911691](/copied/1542448911691.png)

Some methods involved the GCP console and the gcloud command, while others are performed from the OS such as for reboot and shut down. It's important to know that if you are restarting, rebooting, stopping, or even deleting an instance, the shutdown process will take about 90 seconds. For a pre-emptable VM, if the instance does not stop after 30 seconds, compute engine sensing ACPI G3 mechanical off's signal to the operating system. Keep that in mind when writing shutdown scripts for preemptable VMs. 

As I mentioned previously, compute engine can live migrate your virtual machines to another host due to a maintenance event to prevent your applications from experiencing disruptions. 

![1542449065813](/copied/1542449065813.png)



A VMs availability policy determines how the instance behaves in such an event. The default maintenance behavior for instances is to live migrate, but you can change the behavior to terminate your instance during maintenance events instead. If your VM is terminated due to a crash or other maintenance events, your instance automatically restarts by default but this can also be changed. These availability policies can be configured both during the instance creation and while an instance is running by configuring the automatic restart and on host maintenance options.

When a VM is terminated, you do not pay from memory and CPU of resources, however, you are charged for any attached disks and IP addresses.

![1542449171197](/copied/1542449171197.png)



In the terminated state, you can perform any of the actions listed here such as, changing the machine type, but you cannot change the image of a stop VM. Also, not all of the actions listed here require you to stop a virtual machine. For example, VM availability policies can be changed while the VM is running as discussed previously.

#### Lab 1 Creating Virtual Machines

In this lab, you created several virtual machine instances of different types with different characteristics. Specifically, you created a small utility VM for administration purposes, a Windows VM, and a custom Linux VM.

You also accessed both the Windows and Linux VMs and deleted all your created VMs.

In general, start with a smaller VM when you're prototyping solutions to keep the cost down. When you're ready for production, trade up to larger VMs based on capacity. If you're building in redundancy for availability, remember to allocate access capacity to meet performance requirements. Finally, consider using custom VMs when your application requirements fit between the features of standard types.



### Working with Virtual Machines

#### Compute options

Now that you have completed the lab, let's dive deeper into the compute options that are available to you in GCP by focusing on CPU and memory. 

![1542451086484](/copied/1542451086484.png)

When it comes to creating and configuring a VM, you have three options. You can use the Web Console, as you did in the previous lab, the Cloudshell command line, or the RESTful API. If you have very complex configurations that you'd like to automate and process, you might want to programmatically configure this through the RESTful API by defining all the different options for your environments.

If you plan on using the command line or RESTful API, I recommend that you first configure the instance through the Web Console, and then ask compute engine for the equivalent REST request or command line. This way, you avoid any typos and get dropped on lists of all the available CPU and memory options. 

![1542451191523](/copied/1542451191523.png)



Speaking of that, let's look at the different machine types that are currently available. Now, I put this in as a reference chart here to just give you an idea of the different options that you have available. These predefined machine types are managed by compute engine and come in four classes: standard, high memory, high CPU, and shared core memory machines. 

Shared core machines are great for prototyping solutions because they are cost effective for running small non-resource intensive applications. 

The difference between the standard, high memory, and high CPU machine types, is the ratio of number of vCPUs to memory. 

Standard machine types are suitable for tasks that have a balance of CPU and memory needs. Specifically, standard machine types have 3.75 gigs of RAM per virtual CPU, all the way to 64 virtual CPUs with 240 gigs of RAM. 

Next, high memory machine types are ideal for tasks that require more memory relative to virtual CPUs. These machine types have 6.5 gigabytes of RAM per virtual CPU. 

Finally, high CPU machine types are ideal for tasks that require more virtual CPUs relative to memory. These machine types have 0.9 gigabytes of RAM per virtual CPU. 

Now, this table is constantly going to change and in fact some of these configurations aren't available everywhere. So let's explore this by choosing regions and zones. 

![1542451416601](/copied/1542451416601.png)



The first thing you want to consider when choosing a region and zone is the geographical location in which you want to run your resources. However, there are other considerations to keep in mind such as the fact that Google has been consistently and constantly deploying new hardware architectures. These new hardware architectures are not deployed to all zones at the same time. Therefore, you're going to find that there are different CPU architectures in different zones. In fact, there can be up to three generations of difference in those processor types. 

Now what that means to you is that your application might benefit from some of these advanced Intel technologies that have been released. In this case from a billing perspective, nothing changes. You pay the exact same amount regardless of the type of individual CPU processor that you might be using. Google is constantly increasing the number of regions and zones and updating the supported CPU architecture. For an up to date list, refer to the documentation link provided on the slide or go to cloud.google.com and search for regions and zones. 

![1542451573154](/copied/1542451573154.png)



Coming back to machine types. If the predefined machine types don't fit your compute needs, you can always create a custom machine type. You already performed this in the lab, but I want to highlight that you can customize your CPU memory and GPU needs. These custom VMs are generally more expensive than the predefined VMs, but they provide you with more flexibility. 

Now, how does Google give discounts on custom machines?

![1542451632973](/copied/1542451632973.png)

You'll notice that custom machines don't really fall into a category so, we can't lump them together as we've seen so far. Instead for custom machine types, compute engine calculates sustained used discounts based on virtual CPU and memory usage, and applies the discount described by the sustained user's discount table that I showed earlier. For example, consider this scenario on the left where you have two instances that have different shapes and run at different times of the month. Compute engine breaks down the number of visual CPUs and amount of memory used across all custom instances and combines resources to qualify for the biggest sustained usage discounts possible. In this case, compute engine applies the following sustained used discounts as seen on the right. You would receive a 30 percent discount of the cost of using two virtual CPUs, and a 30 percent discount of the cost of using four gigabytes of memory because you used each resource for the whole month. You would also receive a 10 percent cost of using two virtual CPUs, and a 10 percent discount of the cost of two gigabytes of memory because you used each resource for half of the month. In essence, compute engine will aggregate all of the CPU and memory resources, times the amount of time that they were used and apply those sustained used discounts. 

![1542451765428](/copied/1542451765428.png)



As I mentioned earlier, a preemptible VM is an instance that you can create and run at a much lower price than normal instances. See whether you can make your application function completely on preemptible VMs because an 80 percent discount is a significant investment in your application. Now just to reiterate, these VMs might be preempted at any time, and there's no charge if that happens within the first 10 minutes. Also preemptible VMs are only going to live for up to 24 hours and you only get a 30 second notification before the machine is preempted. Also there are no live migrations nor automatic restarts in preemptible VMs, but something that we will highlight is that you can actually create monitoring and load balancers that can start up new preemptible VMs in case of failure. In other words, there are external ways to keep restarting preemptible VMs if you need to. One big used case for preemptible VMs is running a batch processing job. If some of those instances terminate during processing, the job slows down but it does not completely stop. Therefore, preemptible instances complete your batch processing tasks without placing additional workload on your existing instances and without requiring you to pay full price for additional normal instances.

#### Images

Next, let's focus on images. 

![1542472919237](/copied/1542472919237.png)

When creating a virtual machine, you have the option of choosing the boot disk image. This image includes the boot loader, the operating system, the file system structure, any preconfigured software, and any other customizations.

G Cloud is included in every virtual machine image that you choose from Compute Engine.

These images are tar gzip'd files, and they are stored in a private area of Cloud Storage.

![1542473139346](/copied/1542473139346.png)

When choosing an image, you can select either a public or custom image.

As you saw in the previous lab, you can choose from both Linux and Windows images.

Some of these images are premium images as indicated in parentheses with a p.

These images will have per second charges with the exception of SQL Server images, which are charged per minute.

Premium image prices vary with the machine type. However, these prices are global and do not vary by region or zone.

You can also use custom images. For example, you can create a custom image by pre-installing software that's been authorized for your particular organization.

You also have the option of importing images from your own premises or workstation or from another cloud provider. This is a no cost service that is as simple as installing an agent, and I highly recommend that you look into it.

You can also share custom images with anybody in your project or among other projects, too.

#### Disk options

At this point, you've chosen an operating system, but that operating system is going to be included as part of some kind of disk. So, let's look at disk options. 

![1542473186979](/copied/1542473186979.png)

Every single VM comes with a single root persistent disk because you're choosing a base image to have that loaded on. This image is bootable and that you can attach it to a VM and boot from it. And it's durable and that it can survive if the VM terminates. To have a boot to survive a VM deletion, the "Delete boot disk when instance is deleted" box needs to be cleared in the instances property. As I discussed earlier, there are different types of disks.

So, let's explore these in more detail.

![1542473261418](/copied/1542473261418.png)

The first disk that we create is what we call a persistent disk. That means it's going to be attached to the VM through the network interface. Even though it's persistent, it's not physically attached to the VM. This separation of disk and compute allows the disk to survive if the VM terminates. 

However, persistent disks are bounded to their zone and that they can't be moved between zones. You can also perform snapshots of these disks which are incremental backups that we will discuss more later. 

The choice between HDD and SSD disk comes down to cost and performance. To learn more about this, I recommend referring to the documentation link here because performance is improving quite often. 

Another cool feature of persistent disks is that you can dynamically resize them even while they're running and attached to a VM. You can also *attach a disk in read-only mode to multiple VMs*. This allows you to share static data between multiple instances which is cheaper then replicating your data to unique disks for individual instances. 

Now, local SSDs are different from persistent disks and that they're physically attached to the virtual machine. 

![1542473481466](/copied/1542473481466.png)

Therefore, these disks are ephemeral but provide very high ions. For up-to-date numbers, I recommend referring to the documentation. Currently, you can attach up to eight local SSD disks with 375 gigabytes each resulting in a total of three terabytes. Data on these disks will survive a reset but not a VM stop or terminate because these disks can't be reattached to a different VM.

![1542473559547](/copied/1542473559547.png)



You also have the option of using a RAM disk. You can simply use TMPFS if you want to store data in memory. This is going to be the fastest type of performance available if you need small data structures. I recommend a high-memory virtual machine if you need to take advantage of such features along with a persistent disk to back up the RAM disk data.

In summary, you've got a number of different disk options.

![1542473636845](/copied/1542473636845.png)

Persistent disks can be rebooted and snapshoted, but local SSD and RAM disks are ephemeral. I recommend choosing a persistent HDD disk when you don't need performance but just need capacity. If you have high-performance needs, start looking at the SSD options. The persistent SSD disks offer data redundancy because the data on each persistent disks is distributed across several physical disks. Local SSDs provide even higher performance but without the data redundancy. Finally, RAM disks are very volatile, but they provide the highest performance.

Now, just as there is a limit on how many local SSDs you can attach to VM, there is also limit on how many persistent disks you can attach to a VM.

![1542473714144](/copied/1542473714144.png)



As illustrated in this table, this limit depends on the number of cores of the VM. For example, if your machine has eight or more cores, you can attach up to 128 disks. So you can create massive amounts of capacity for a single host. Now, remember that little nuance when I told you about how throughput is limited by the number of cores that you have? That throughput also shares the same bandwidth with this scale.

So, if you plan on doing a large amount of this kind of throughput, it will also compete with any network egress or ingress throughput. So, remember that especially if you're going to be increasing the number of drives attached to a virtual machine. 

There are many differences between a physical hard disk in a computer and a persistent disk, which is essentially a virtual network device. 

![1542473828569](/copied/1542473828569.png)



First of all, if you remember, with normal computer hardware disks, you have to partition them. Essentially, you have a drive and you're carving up a section for the operating system to get its own capacity. If you want to grow it, you have to repartition and if you want to make changes, you might even have to reformat. If you want redundancy, you might create a redundant disk array and if you want encryption, you need to encrypt files before writing them to the disk. With cloud-persistent disks, things are very different because all the management is handled for you on the backend. You can simply grow disks and resize the file system because disks are a virtual network devices. Redundancy and snapshot services are built in and disks are automatically encrypted. You can even use your own keys and that will ensure that no party can get to the data except you.

#### Common Compute Engine actions

Now that we have covered all the different compute, image and disk options, let's look at some common actions that you can perform with Compute Engine. 

One common action might be moving an instance to a new zone. 

![1542473994449](/copied/1542473994449.png)



For example, you might do so for geographical reasons or because a zone is deprecated. If you move your instance within the same region, you can automate the move by using the gcloud compute instances move command. 

![1542474038748](/copied/1542474038748.png)



If you move your instance to a different region, you need to manually do so by following the process outlined here. This involves making a snapshot of all persistent disks, and creating new disks in the destination zone from that snapshot. Next, you create the new VM in the destination zone and attach the new persistent disks, assign a static IP and update any references to the VM. Finally, you delete the original VM, its disks and the snapshot. 

Speaking of snapshots, let's take a closer look at these. 

![1542474110243](/copied/1542474110243.png)



Snapshots have many use cases. For example, they can be used to backup critical data into a durable storage solution to meet application, availability and recovery requirements. These snapshots are stored in cloud storage, which we'll cover in a later course of the specialization.

Snapshots can also be used to migrate data between zones. 

![1542474172647](/copied/1542474172647.png)



I just discussed this when going over the manual process of moving an instance between two regions, but this can also be used to simply transfer data from one zone to another. For example, you might want to minimize latency by migrating data to a drive that can be locally attached in the zone where it is used. Which brings me to another snapshot use case of transferring data to a different disk type. 

![1542474232312](/copied/1542474232312.png)



For example, if you want to improve disk performance, you could use a snapshot to transfer data from a standard HDD persistent disk to a SSD persistent disk.

Now that I've covered some of these snapshot use cases, let's explore the concept of a disk snapshot.

![1542474282241](/copied/1542474282241.png)



First of all, this slide is titled persistent disk snapshots because snapshots are available only to persistent disks and not to local SSDs. Snapshots are different from public images and custom images, which are used primarily to create instances or configure instance templates, and that snapshots are useful for periodic backup of the data on your persistent disks. Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of the disk. As we saw with the previous examples, snapshots can be restored to a new persistent disk allowing for a move to a new zone.

![1542474381873](/copied/1542474381873.png)



Before taking a disk snapshot, you should prepare the persistent disk. You can create a snapshot of a persistent disk even while your applications write data to that disk. However, I recommend freezing or unmounting the file system before you take a snapshot. This is the safest and most reliable way to ensure that your disk buffers are cleared which improves snapshot consistency. Specifically, when connected to your instance using SSH, stop any applications that are reading or writing data to the persistent disk and either freeze a file system or unmount the file system. To freeze, use the command, sudo fsfreeze and to unmount, use sudo unmount as shown on the slide. 

Another common compute action is to resize your persistent disk.

![1542474462501](/copied/1542474462501.png)



The main benefit of increasing storage capacity is to improve I/O performance as illustrated in this table. This can be achieved while the disk is attached to a running VM without having to create a snapshot. Now, while you can grow disks in size, you can never shrink them, so do keep this in mind.

#### Lab 2 Working with Virtual Machines 

Let's get started with the second lab of this module. In this lab, you'll be setting up an Application server. Now, this example happens to be a gaming application, but it applies to many other use cases. You won't just configure the VM, but you will also add capacity for production gaming system, and you will build the infrastructure that you need for production activities. This includes backups and graceful shutdown and restart services. The lab will take about 45 minutes to complete, but you have 90 minutes before it turns out.

### Review

In this module, we cover the different compute, image, and disk options within Compute Engine, along with some common actions. Remember that by default, VMs have a single network interface with the local internal IP address. The external IP address is mapped to the internal IP address by a Network Address Translation Service. So, the VM itself knows nothing about the external IP. Also, they default disk type as a persistent disk which is network attached to the VM. You can get higher performance of these types but there is a trade off because you lose many of the services that are provided by persistent disks. Finally, remember that many of the common things you can do with physical disks might be a duplication of features already provided automatically by the disk service. So either there are unnecessary or they won't provide the benefit that you expect





